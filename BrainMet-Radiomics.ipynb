{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Source Code\n",
    "\n",
    "accompanying the paper: \n",
    "\n",
    "**Limited Capability of MRI Radiomics to Predict Primary Tumor Histology of Brain Metastases in External Validation**  \n",
    "\n",
    "Neuro-Oncology Advances 2024  \n",
    "\n",
    "DOI: 10.1093/noajnl/vdae060   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Naming Convention: \n",
    "_0000: FLAIR  \n",
    "_0001: T1  \n",
    "_0002: T1CE     \n",
    "\n",
    "### Segmentation Labels:    \n",
    "0: background    \n",
    "1: edema    \n",
    "2: necrotic tissue    \n",
    "3: enhancing tumor   \n",
    "\n",
    "### Data Structure:\n",
    "```bash\n",
    "/BrainMetDataset/  \n",
    "├── data.xlsx (containing case IDs)    \n",
    "├── Pat01    \n",
    "│   ├── Pat01_0000.nii.gz (FLAIR)  \n",
    "│   ├── Pat01_0001.nii.gz (T1)  \n",
    "│   ├── Pat01_0002.nii.gz (T1CE)  \n",
    "├── Pat02  \n",
    "│   ├── Pat02_0000.nii.gz (FLAIR)  \n",
    "│   ├── Pat02_0001.nii.gz (T1)  \n",
    "│   ├── Pat02_0002.nii.gz (T1CE)  \n",
    "├── Pat03  \n",
    "│   ├── ...  \n",
    "```\n",
    "\n",
    "### Requirements\n",
    "- SimpleITK\n",
    "- numpy\n",
    "- nipype\n",
    "- fsl\n",
    "- ants\n",
    "- HD-BET\n",
    "- connected-components-3d (cc3d)\n",
    "- sklearn\n",
    "- imblearn\n",
    "- tqdm\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- intensity_normalization\n",
    "- multiprocessing\n",
    "- pandas \n",
    "- nnunetv1\n",
    "- scipy\n",
    "- pyradiomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Preprocessing functions can be found under brainmet.utils.preprocessing\n",
    "\n",
    "Steps:\n",
    "1. DICOM to NIFTI conversion: dcm2niix_folder\n",
    "2. Brain Extraction: extract_brain\n",
    "3. Cropping: get_bounding_box, apply_bounding_box\n",
    "4. Bias Correction: apply_bias_correction\n",
    "5. Co-Registration: coregister_nipype or coregister_antspy\n",
    "6. Intensity Normalization: zscore_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tumor Segmentation\n",
    "- Using a pretrained nnU-Net, e.g., Task001_BrainTumour (trained on glioma data, but also works for brain metastases)\n",
    "- **CAVE: Segmentations need to be manually verified and corrected!**\n",
    "- Tumor segmentation before N4Bias-Correction and Normalization may give better results with the pretrained nnU-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunet.inference import predict\n",
    "import os\n",
    "from brainmet.utils.config import DATASET_PATH\n",
    "\n",
    "os.makedirs(os.path.join(DATASET_PATH, \"segmented\"), exist_ok=True)\n",
    "\n",
    "predict.predict_from_folder(\n",
    "    input_folder=os.path.join(DATASET_PATH, \"preprocessed/\"),\n",
    "    output_folder=os.path.join(DATASET_PATH, \"segmented/\"),\n",
    "    part_id=0,\n",
    "    num_parts=1,\n",
    "    model=\"/PATH/TO/NNUNET/nnUNet_trained_models/nnUNet/3d_fullres/Task001_BrainTumour/nnUNetTrainerV2__nnUNetPlansv2.1/\",\n",
    "    folds=\"all\",\n",
    "    save_npz=False,\n",
    "    lowres_segmentations=None,\n",
    "    num_threads_preprocessing=2,\n",
    "    num_threads_nifti_save=2,\n",
    "    tta=True,\n",
    "    step_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Tumor Masks\n",
    "- We will only use the tumor core (necrotic and enhancing tumor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install connected-components-3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc3d\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from brainmet.utils.preprocessing import binarize_itkimage\n",
    "from brainmet.utils.config import DATASET_PATH\n",
    "\n",
    "\n",
    "cases = [\"LIST\", \"OF\", \"CASE_IDs\"]\n",
    "met_numbers = []\n",
    "\n",
    "for c in tqdm(cases):\n",
    "    met_number = {}\n",
    "\n",
    "    img = sitk.ReadImage(\n",
    "        os.path.join(DATASET_PATH, c, f\"{c}_NAME_OF_SEGMENTATION.nii.gz\")\n",
    "    )\n",
    "    img_bin = binarize_itkimage(img, l_thresh=2, u_thresh=3)\n",
    "\n",
    "    arr = sitk.GetArrayFromImage(img_bin).astype(int)\n",
    "\n",
    "    labels_out, n = cc3d.connected_components(arr, return_N=True, connectivity=26)\n",
    "\n",
    "    for label, image in cc3d.each(labels_out, binary=False, in_place=True):\n",
    "        image = np.where(image == label, 1, 0)\n",
    "        image = image.astype(np.uint8)\n",
    "        img_new = sitk.GetImageFromArray(image)\n",
    "        img_new.CopyInformation(img)\n",
    "        # Each metastasis is saved in a separate numbered label image\n",
    "        sitk.WriteImage(\n",
    "            img_new,\n",
    "            os.path.join(DATASET_PATH, c, f\"{c}_MASK_NAME_NUMBER_{label}.nii.gz\"),\n",
    "        )\n",
    "\n",
    "    met_number[\"ID\"] = c\n",
    "    met_number[\"number\"] = n\n",
    "    met_numbers.append(met_number)\n",
    "\n",
    "met_numbers_df = pd.DataFrame(met_numbers)\n",
    "met_numbers_df.to_excel(os.path.join(DATASET_PATH, \"met_numbers.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import radiomics\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from collections import OrderedDict\n",
    "from brainmet.utils.config import DATASET_PATH\n",
    "\n",
    "N_PROC = multiprocessing.cpu_count() - 1\n",
    "\n",
    "# List that contains the number of metastases for each case\n",
    "number = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"met_numbers.xlsx\"), header=0, index_col=None\n",
    ")\n",
    "sequences = [\n",
    "    (\"FLAIR\", \"0001\"),\n",
    "    (\"T1SE\", \"0002\"),\n",
    "    (\"T1CE\", \"0003\"),\n",
    "]\n",
    "\n",
    "radiomics.setVerbosity(60)\n",
    "\n",
    "loopList = zip(number.ID.to_list(), number.number.to_list())\n",
    "\n",
    "params = \"Params.yaml\"\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "failed = manager.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(x):\n",
    "    \"\"\"\n",
    "    Function to extract radiomic features for each case separately and\n",
    "    store in case folder\n",
    "\n",
    "    Args:\n",
    "    Tuple: (case_id, number_of_metastases)\n",
    "    \"\"\"\n",
    "\n",
    "    p = x[0]\n",
    "    i = x[1]\n",
    "\n",
    "    global failed\n",
    "\n",
    "    result_list = []\n",
    "    images = []\n",
    "\n",
    "    # Load all structural images for case\n",
    "    for sequenceName, sequenceNumber in sequences:\n",
    "        itkimage = sitk.ReadImage(\n",
    "            os.path.join(DATASET_PATH, p, f\"{p}_IMAGE_NAME_{sequenceNumber}.nii.gz\")\n",
    "        )\n",
    "        images.append((itkimage, sequenceName))\n",
    "\n",
    "    # Extract features for each metastasis separately\n",
    "    for j in range(1, int(i) + 1):\n",
    "        try:\n",
    "            res = OrderedDict()\n",
    "            res[\"ID\"] = f\"{p}_{j}\"\n",
    "\n",
    "            maskImage = sitk.ReadImage(\n",
    "                os.path.join(DATASET_PATH, p, f\"{p}_MASK_NAME_NUMBER_{j}.nii.gz\")\n",
    "            )\n",
    "\n",
    "            for img, name in images:\n",
    "                extractor = radiomics.featureextractor.RadiomicsFeatureExtractor(params)\n",
    "                result = extractor.execute(img, maskImage, label=1)\n",
    "\n",
    "                # Add Sequence name as prefix to each Key\n",
    "                result = {f\"{name}_{k}\": v for k, v in result.items()}\n",
    "\n",
    "                # Remove all diagnostics keys\n",
    "                result = {k: v for k, v in result.items() if not \"diagnostics\" in k}\n",
    "\n",
    "                res.update(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            failed.append(f\"{p}_{j}: {str(e)}\")\n",
    "\n",
    "        result_list.append(res)\n",
    "\n",
    "    result_table = pd.DataFrame(result_list)\n",
    "    # keep each shape feature only once\n",
    "    result_table.columns = result_table.columns.str.replace(\".*shape\", \"shape\")\n",
    "    result_table = result_table.loc[:, ~result_table.columns.duplicated()]\n",
    "    result_table.to_excel(os.path.join(DATASET_PATH, p, \"features.xlsx\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    r = process_map(get_features, loopList, max_workers=N_PROC)\n",
    "\n",
    "    # Save log of instances where extraction failed\n",
    "    with open(os.path.join(DATASET_PATH, \"Feature_extraction_log.txt\"), \"w\") as file:\n",
    "        for row in failed:\n",
    "            s = \"\".join(map(str, row))\n",
    "            file.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResults = pd.DataFrame()\n",
    "\n",
    "for p in tqdm(number.ID.to_list()):\n",
    "    data = pd.read_excel(\n",
    "        os.path.join(DATASET_PATH, p, \"features.xlsx\"), header=0, index_col=0\n",
    "    )\n",
    "    finalResults = pd.concat([finalResults, data], axis=0)\n",
    "\n",
    "\n",
    "finalResults.dropna(inplace=True)\n",
    "finalResults.sort_index(axis=1, inplace=True)\n",
    "finalResults.sort_index(axis=0, inplace=True)\n",
    "\n",
    "finalResults.to_excel(os.path.join(DATASET_PATH, \"features_complete.xlsx\"))\n",
    "print(finalResults.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Location of Metastases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "loopList = zip(number.ID.to_list(), number.number.to_list())\n",
    "\n",
    "data = []\n",
    "\n",
    "for p, i in tqdm(loopList):\n",
    "    for j in range(1, int(i) + 1):\n",
    "        maskName = os.path.join(DATASET_PATH, p, f\"{p}_MASK_NAME_NUMBER_{j}.nii.gz\")\n",
    "        itk_image = sitk.ReadImage(maskName)\n",
    "\n",
    "        image_array = sitk.GetArrayFromImage(itk_image)\n",
    "\n",
    "        # in z,y,x direction\n",
    "        CM = ndimage.measurements.center_of_mass(np.array(image_array))\n",
    "\n",
    "        relativeCOM = np.array(CM) / np.array(image_array.shape)\n",
    "\n",
    "        data.append([f\"{p}_{j}\", relativeCOM[2], relativeCOM[1], relativeCOM[0]])\n",
    "\n",
    "result_table = pd.DataFrame(data, columns=[\"ID\", \"x\", \"y\", \"z\"])\n",
    "\n",
    "result_table.to_excel(os.path.join(DATASET_PATH, \"center_of_mass.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"features_complete.xlsx\"), header=0, index_col=None\n",
    ")\n",
    "location = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"center_of_mass.xlsx\"), header=0, index_col=None\n",
    ")\n",
    "features.set_index(\"ID\", inplace=True)\n",
    "location.set_index(\"ID\", inplace=True)\n",
    "\n",
    "features.loc[\n",
    "    features.index.intersection(location.index), [\"x\", \"y\", \"z\"]\n",
    "] = location.loc[features.index.intersection(location.index), [\"x\", \"y\", \"z\"]]\n",
    "\n",
    "features.to_excel(os.path.join(DATASET_PATH, \"features_location_complete.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Classifier Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_validate,\n",
    "    StratifiedGroupKFold,\n",
    "    permutation_test_score,\n",
    ")\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import os\n",
    "import random\n",
    "from brainmet.utils.feature_selection import MRMRFeatureSelector, LASSO_selection\n",
    "from brainmet.utils.config import RESULT_PATH, DATASET_PATH\n",
    "from brainmet.utils.plotting import (\n",
    "    plot_distribution,\n",
    "    plot_heatmap,\n",
    "    plot_multiclass_roc,\n",
    "    plot_pca,\n",
    ")\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "import math\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.color\": \"black\",\n",
    "        \"axes.labelcolor\": \"black\",\n",
    "        \"font.weight\": \"normal\",\n",
    "        \"font.size\": 16,\n",
    "        \"xtick.color\": \"black\",\n",
    "        \"ytick.color\": \"black\",\n",
    "    }\n",
    ")\n",
    "\n",
    "RANDOM_SEED = 1\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "selected_classes = [\n",
    "    \"Breast Cancer\",\n",
    "    \"Colorectal Cancer\",\n",
    "    \"Lung Cancer\",\n",
    "    \"Melanoma\",\n",
    "    \"Other\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tables\n",
    "- Tables contain radiomic and location features for each metastasis and a column 'Primary' containing the histology\n",
    "- Index = caseID (to group all metastases of a patient to either train or test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Data\n",
    "data_train = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"features_with_location_primary_train.xlsx\"),\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    dtype={\"Primary\": str},\n",
    ")\n",
    "\n",
    "# Test Data\n",
    "data_test_1 = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"features_with_location_primary_test1.xlsx\"),\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    dtype={\"Primary\": str},\n",
    ")\n",
    "\n",
    "data_test_2 = pd.read_excel(\n",
    "    os.path.join(DATASET_PATH, \"features_with_location_primary_test2.xlsx\"),\n",
    "    header=0,\n",
    "    index_col=0,\n",
    "    dtype={\"Primary\": str},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "X_train = data_train.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "Y_train = data_train[\"Primary\"]\n",
    "\n",
    "# Group all metastases of a patient into either train or test set\n",
    "train_inds, test_inds = next(\n",
    "    StratifiedGroupKFold(n_splits=5).split(X_train, Y_train, groups=X_train.index)\n",
    ")\n",
    "\n",
    "X_train_int = X_train.iloc[train_inds]\n",
    "X_test_int = X_train.iloc[test_inds]\n",
    "Y_train_int = Y_train.iloc[train_inds]\n",
    "Y_test_int = Y_train.iloc[test_inds]\n",
    "\n",
    "norm_options = [StandardScaler(), MinMaxScaler()]\n",
    "\n",
    "# with l1 (=LASSO) regularization\n",
    "lasso_fs = SelectFromModel(\n",
    "    LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1000),\n",
    "    max_features=5,\n",
    ")\n",
    "\n",
    "feature_selection_options = [\n",
    "    MRMRFeatureSelector(number=5),\n",
    "    SelectKBest(k=5),\n",
    "    lasso_fs,\n",
    "]\n",
    "\n",
    "oversampling_methods = [\n",
    "    None,\n",
    "    SMOTE(random_state=RANDOM_SEED, k_neighbors=4),\n",
    "    RandomOverSampler(random_state=RANDOM_SEED),\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(probability=True),\n",
    "    NuSVC(nu=0.01, probability=True),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier(),\n",
    "]\n",
    "\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc_ovo\",\n",
    "    \"f1\": \"f1_macro\",\n",
    "}\n",
    "\n",
    "for norm in norm_options:\n",
    "    for fs in feature_selection_options:\n",
    "        for ovs in oversampling_methods:\n",
    "            for clf in classifiers:\n",
    "                steps = []\n",
    "\n",
    "                steps.append((\"scaler\", norm))\n",
    "                steps.append((\"selector\", fs))\n",
    "                if os:\n",
    "                    steps.append((\"oversampler\", ovs))\n",
    "                steps.append((\"classifier\", clf))\n",
    "                # Using imbpipeline to ensure only on training data is oversampled\n",
    "                pipe = imbpipeline(steps)\n",
    "\n",
    "                scores = cross_validate(\n",
    "                    pipe,\n",
    "                    X_train_int,\n",
    "                    Y_train_int,\n",
    "                    scoring=scoring,\n",
    "                    cv=StratifiedGroupKFold(n_splits=5),\n",
    "                    groups=X_train_int.index,\n",
    "                    return_train_score=True,\n",
    "                )\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"normalization\": norm.__class__.__name__,\n",
    "                        \"feature_selection\": fs.__class__.__name__,\n",
    "                        \"oversampling\": ovs.__class__.__name__ if os else \"None\",\n",
    "                        \"classifier\": clf.__class__.__name__,\n",
    "                        \"accuracy\": round(scores[\"test_acc\"].mean(), 3),\n",
    "                        \"ROC_AUC_macro\": round(scores[\"test_roc_auc\"].mean(), 3),\n",
    "                        \"F1_macro\": round(scores[\"test_f1\"].mean(), 3),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(os.path.join(RESULT_PATH, \"results_select_classifier.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Sequential Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "X_train = data_train.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "Y_train = data_train[\"Primary\"]\n",
    "\n",
    "# Group all metastases of a patient into either train or test set\n",
    "train_inds, test_inds = next(\n",
    "    StratifiedGroupKFold(n_splits=5).split(X_train, Y_train, groups=X_train.index)\n",
    ")\n",
    "\n",
    "X_train_int = X_train.iloc[train_inds]\n",
    "X_test_int = X_train.iloc[test_inds]\n",
    "Y_train_int = Y_train.iloc[train_inds]\n",
    "Y_test_int = Y_train.iloc[test_inds]\n",
    "\n",
    "oversampling_methods = [\n",
    "    None,\n",
    "    SMOTE(random_state=RANDOM_SEED, k_neighbors=4),\n",
    "    RandomOverSampler(random_state=RANDOM_SEED),\n",
    "]\n",
    "\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"roc_auc\": \"roc_auc_ovo\",\n",
    "    \"f1\": \"f1_macro\",\n",
    "}\n",
    "\n",
    "for i in range(1, math.ceil(0.01 * len(X_train_int.columns))):\n",
    "    for ovs in oversampling_methods:\n",
    "        steps = []\n",
    "\n",
    "        lasso_fs = SelectFromModel(\n",
    "            LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1000),\n",
    "            max_features=i,\n",
    "        )\n",
    "\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "        steps.append((\"selector\", lasso_fs))\n",
    "        if os:\n",
    "            steps.append((\"oversampler\", ovs))\n",
    "        steps.append((\"classifier\", RandomForestClassifier(random_state=RANDOM_SEED)))\n",
    "        pipe = imbpipeline(steps)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            pipe,\n",
    "            X_train_int,\n",
    "            Y_train_int,\n",
    "            scoring=scoring,\n",
    "            cv=StratifiedGroupKFold(n_splits=5),\n",
    "            groups=X_train_int.index,\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Feature_Selection\": \"Lasso\",\n",
    "                \"n_features\": i,\n",
    "                \"oversampling\": ovs.__class__.__name__ if os else \"None\",\n",
    "                \"classifier\": \"Random_Forest\",\n",
    "                \"accuracy\": round(scores[\"test_acc\"].mean(), 3),\n",
    "                \"ROC_AUC_macro\": round(scores[\"test_roc_auc\"].mean(), 3),\n",
    "                \"F1_macro\": round(scores[\"test_f1\"].mean(), 3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(\n",
    "    os.path.join(RESULT_PATH, \"results_sequential_feature_selection.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "- all dataset combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_combinations = [\n",
    "    (\n",
    "        \"Train = Local, Test = STANFORD+USCF\",\n",
    "        data_train,\n",
    "        pd.concat([data_test_1, data_test_2]),\n",
    "    ),\n",
    "    (\n",
    "        \"Train = STANFORD, Test = Local+USCF\",\n",
    "        data_test_1,\n",
    "        pd.concat([data_train, data_test_2]),\n",
    "    ),\n",
    "    (\n",
    "        \"Train = UCSF, Test = Local+STANFORD\",\n",
    "        data_test_2,\n",
    "        pd.concat([data_train, data_test_1]),\n",
    "    ),\n",
    "    (\n",
    "        \"Train = Local+STANFORD, Test = UCSF\",\n",
    "        pd.concat([data_train, data_test_1]),\n",
    "        data_test_2,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = Local+UCSF, Test = STANFORD\",\n",
    "        pd.concat([data_train, data_test_2]),\n",
    "        data_test_1,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = STANFORD+UCSF, Test = Local\",\n",
    "        pd.concat([data_test_1, data_test_2]),\n",
    "        data_train,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = Local, Test = STANFORD\",\n",
    "        data_train,\n",
    "        data_test_1,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = Local, Test = USCF\",\n",
    "        data_train,\n",
    "        data_test_2,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = STANFORD, Test = Local\",\n",
    "        data_test_1,\n",
    "        data_train,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = STANFORD, Test = UCSF\",\n",
    "        data_test_1,\n",
    "        data_test_2,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = UCSF, Test = Local\",\n",
    "        data_test_2,\n",
    "        data_train,\n",
    "    ),\n",
    "    (\n",
    "        \"Train = UCSF, Test = Stanford\",\n",
    "        data_test_2,\n",
    "        data_test_1,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Correct\" Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "N_FEATURES = \"NUMBER_OF_FEATURES_TO_SELECT\"\n",
    "\n",
    "for combination_name, train_data, test_data in dataset_combinations:\n",
    "    # select columns for X and Y\n",
    "    X_train = train_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "    Y_train = train_data[\"Primary\"]\n",
    "\n",
    "    X_test = test_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "    Y_test = test_data[\"Primary\"]\n",
    "\n",
    "    # Create Train/Test Split for Train Data, grouping metastases\n",
    "    # of the same patient in either train or test set\n",
    "    train_inds, test_inds = next(\n",
    "        StratifiedGroupKFold(n_splits=5).split(X_train, Y_train, groups=X_train.index)\n",
    "    )\n",
    "\n",
    "    X_train_int = X_train.iloc[train_inds]\n",
    "    X_test_int = X_train.iloc[test_inds]\n",
    "    Y_train_int = Y_train.iloc[train_inds]\n",
    "    Y_test_int = Y_train.iloc[test_inds]\n",
    "\n",
    "    X_test_ext = X_test.copy()\n",
    "    Y_test_ext = Y_test.copy()\n",
    "\n",
    "    # Feature Normalization\n",
    "    # fit only on train data, apply transform to test data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_int = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train_int), columns=X_train_int.columns\n",
    "    )\n",
    "    X_test_int = pd.DataFrame(scaler.transform(X_test_int), columns=X_test_int.columns)\n",
    "    X_test_ext = pd.DataFrame(scaler.transform(X_test_ext), columns=X_test_ext.columns)\n",
    "\n",
    "    # Select features based on training data\n",
    "    features = LASSO_selection(X_train_int, Y_train_int, N_FEATURES)\n",
    "    X_train_int = X_train_int.loc[:, features]\n",
    "    X_test_int = X_test_int.loc[:, features]\n",
    "    X_test_ext = X_test_ext.loc[:, features]\n",
    "\n",
    "    # Oversampling\n",
    "    SMOTE_Sampler = SMOTE(random_state=RANDOM_SEED, k_neighbors=4)\n",
    "    X_train_int_SMOTE, Y_train_int_SMOTE = SMOTE_Sampler.fit_resample(\n",
    "        X_train_int, Y_train_int\n",
    "    )\n",
    "\n",
    "    ROS = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "    X_train_int_ROS, Y_train_int_ROS = ROS.fit_resample(X_train_int, Y_train_int)\n",
    "\n",
    "    dataframes = [\n",
    "        (X_train_int, Y_train_int, \"Baseline\"),\n",
    "        (X_train_int_SMOTE, Y_train_int_SMOTE, \"SMOTE Correct Oversampling\"),\n",
    "        (X_train_int_ROS, Y_train_int_ROS, \"ROS Correct Oversampling\"),\n",
    "    ]\n",
    "\n",
    "    # Plot test set distributions\n",
    "    plot_distribution(\n",
    "        Y_test_int, \"Internal Test Set Correct Oversampling\", combination_name\n",
    "    )\n",
    "\n",
    "    plot_distribution(\n",
    "        Y_test_ext, \"External Test Set Correct Oversampling\", combination_name\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "    result[\"Combination_Name\"] = combination_name\n",
    "\n",
    "    for X, Y, name in dataframes:\n",
    "        # Plot Label Distribution for Train Set\n",
    "        plot_distribution(Y, f\"Internal Train Set {name}\", combination_name)\n",
    "\n",
    "        print(Y.value_counts(sort=True, normalize=False))\n",
    "\n",
    "        plot_pca(\n",
    "            X,\n",
    "            Y,\n",
    "            selected_classes,\n",
    "            f\"Internal Train Set {name}\",\n",
    "            combination_name,\n",
    "            kernel_type=\"rbf\",\n",
    "            gamma_type=1,\n",
    "            alpha_type=0.1,\n",
    "        )\n",
    "\n",
    "        # Model selection with Cross-Validation\n",
    "        cv_inner = StratifiedGroupKFold(n_splits=5)\n",
    "        model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "        space = {\n",
    "            \"n_estimators\": [10, 100, 500, 1000],\n",
    "            \"max_features\": [\"sqrt\", \"auto\", 2, 4, 6],\n",
    "        }\n",
    "\n",
    "        search = GridSearchCV(model, space, scoring=\"f1_macro\", cv=cv_inner, refit=True)\n",
    "\n",
    "        res = search.fit(X, Y, groups=X.index)\n",
    "        # get the best performing model fit on the whole training set\n",
    "        best_model = res.best_estimator_\n",
    "        best_model.fit(X, Y)\n",
    "\n",
    "        # Calculate permutation score to test vs. random labels\n",
    "        score, perm_scores, pvalue = permutation_test_score(\n",
    "            best_model, X, Y, n_permutations=100, n_jobs=4, scoring=\"f1_macro\"\n",
    "        )\n",
    "\n",
    "        # Feature Importance\n",
    "        sorted_idx = best_model.feature_importances_.argsort()\n",
    "        plt.barh(\n",
    "            X.columns[sorted_idx],\n",
    "            best_model.feature_importances_[sorted_idx],\n",
    "            color=\"#083471\",\n",
    "        )\n",
    "        plt.xlabel(f\"{name} Feature Importance\")\n",
    "        plt.savefig(\n",
    "            os.path.join(\n",
    "                RESULT_PATH, f\"Feature_Importance_{combination_name}_{name}.tiff\"\n",
    "            ),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        predictions_int = pd.Series(\n",
    "            pd.Categorical(best_model.predict(X_test_int), categories=selected_classes)\n",
    "        )\n",
    "        probabilities_int = best_model.predict_proba(X_test_int)\n",
    "        ytest_dummies_int = pd.get_dummies(Y_test_int, drop_first=False).to_numpy()\n",
    "        Y_test_int = pd.Series(\n",
    "            pd.Categorical(Y_test_int, categories=selected_classes),\n",
    "            index=Y_test_int.index,\n",
    "        )\n",
    "\n",
    "        conf_matrix_int = confusion_matrix(\n",
    "            Y_test_int, predictions_int, labels=selected_classes\n",
    "        )\n",
    "        array_int = np.array(conf_matrix_int)\n",
    "\n",
    "        plot_heatmap(\n",
    "            normalized_array=array_int,\n",
    "            classes=best_model.classes_,\n",
    "            conf_matrix=conf_matrix_int,\n",
    "            name=f\"{combination_name} Internal Test Set {name}\",\n",
    "            Y=Y_test_int,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        plot_multiclass_roc(\n",
    "            Y_test_int,\n",
    "            probabilities_int,\n",
    "            n_classes=len(selected_classes),\n",
    "            figsize=(6, 5),\n",
    "            name=f\"{combination_name} Internal Test Set {name}\",\n",
    "            classes=best_model.classes_,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "        result[f\"{name}_Length_Train\"] = np.shape(Y)[0]\n",
    "        result[f\"{name}_Length_Test_int\"] = np.shape(Y_test_int)[0]\n",
    "        result[f\"{name}_Length_Test_ext\"] = np.shape(Y_test_ext)[0]\n",
    "        result[f\"{name}_Cross_Validation_Score\"] = np.round(search.best_score_, 2)\n",
    "        result[f\"{name}_Permutation_pvalue\"] = np.round(pvalue, 4)\n",
    "        result[f\"{name}_Hyperparameters\"] = search.best_params_\n",
    "        result[f\"{name}_Features_Number\"] = len(features)\n",
    "        result[f\"{name}_Features\"] = features\n",
    "        result[f\"{name}_Model\"] = \"Random Forest Classifier\"\n",
    "        result[f\"{name}_Pred_Test_int\"] = predictions_int\n",
    "        result[f\"{name}_Proba_Test_int\"] = probabilities_int\n",
    "        result[f\"{name}_ACC_int\"] = np.round(\n",
    "            accuracy_score(Y_test_int, predictions_int), 2\n",
    "        )\n",
    "        result[f\"{name}_F1_int\"] = np.round(\n",
    "            f1_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_PREC_int\"] = np.round(\n",
    "            precision_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_REC_int\"] = np.round(\n",
    "            recall_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_AUC_macro_ovo_int\"] = np.round(\n",
    "            roc_auc_score(\n",
    "                Y_test_int,\n",
    "                probabilities_int[:, 1],\n",
    "                average=\"macro\",\n",
    "                multi_class=\"ovo\",\n",
    "                labels=sorted(selected_classes),\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        result[f\"{name}_log_loss_int\"] = log_loss(\n",
    "            Y_test_int, probabilities_int, labels=sorted(selected_classes)\n",
    "        )\n",
    "\n",
    "        predictions_ext = best_model.predict(X_test_ext)\n",
    "        probabilities_ext = best_model.predict_proba(X_test_ext)\n",
    "        ytest_dummies_ext = pd.get_dummies(Y_test_ext, drop_first=False).to_numpy()\n",
    "\n",
    "        conf_matrix_ext = confusion_matrix(\n",
    "            Y_test_ext, predictions_ext, labels=selected_classes\n",
    "        )\n",
    "        array_ext = np.array(conf_matrix_ext)\n",
    "\n",
    "        plot_heatmap(\n",
    "            normalized_array=array_ext,\n",
    "            classes=best_model.classes_,\n",
    "            conf_matrix=conf_matrix_ext,\n",
    "            name=f\"{combination_name} external test set {name}\",\n",
    "            Y=Y_test_ext,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        plot_multiclass_roc(\n",
    "            Y_test_ext,\n",
    "            probabilities_ext,\n",
    "            n_classes=len(selected_classes),\n",
    "            figsize=(6, 5),\n",
    "            name=f\"{combination_name} external test set {name}\",\n",
    "            classes=best_model.classes_,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        result[f\"{name}_Conf_Matrix_ext\"] = conf_matrix_ext\n",
    "        result[f\"{name}_Pred_Test_ext\"] = predictions_ext\n",
    "        result[f\"{name}_Proba_Test_ext\"] = probabilities_ext\n",
    "        result[f\"{name}_ACC_ext\"] = np.round(\n",
    "            accuracy_score(Y_test_ext, predictions_ext), 2\n",
    "        )\n",
    "        result[f\"{name}_F1_ext\"] = np.round(\n",
    "            f1_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_PREC_ext\"] = np.round(\n",
    "            precision_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_REC_ext\"] = np.round(\n",
    "            recall_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_AUC_macro_ovo_ext\"] = np.round(\n",
    "            roc_auc_score(\n",
    "                Y_test_ext,\n",
    "                probabilities_ext[:, 1],\n",
    "                average=\"macro\",\n",
    "                multi_class=\"ovo\",\n",
    "                labels=sorted(selected_classes),\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        result[f\"{name}_log_loss_ext\"] = log_loss(\n",
    "            Y_test_ext, probabilities_ext, labels=sorted(selected_classes)\n",
    "        )\n",
    "        result[f\"{name}_Conf_Matrix_ext\"] = conf_matrix_ext\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.to_excel(os.path.join(RESULT_PATH, \"results_Correct_Oversampling.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Incorrect\" Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "N_FEATURES = \"NUMBER_OF_FEATURES_TO_SELECT\"\n",
    "\n",
    "for combination_name, train_data, test_data in dataset_combinations:\n",
    "    # select columns for X and Y\n",
    "    X_train = train_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "    Y_train = train_data[\"Primary\"]\n",
    "\n",
    "    X_test = test_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "    Y_test = test_data[\"Primary\"]\n",
    "\n",
    "    # Oversampling incorrectly performed before the dataset split\n",
    "    SMOTE_Sampler = SMOTE(random_state=RANDOM_SEED)\n",
    "    X_train_SMOTE, Y_train_SMOTE = SMOTE_Sampler.fit_resample(X_train, Y_train)\n",
    "\n",
    "    ROS = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "    X_train_ROS, Y_train_ROS = ROS.fit_resample(X_train, Y_train)\n",
    "\n",
    "    # Plot external test set distribution\n",
    "    plot_distribution(Y_test, \"External Test Set Wrong Oversampling\", combination_name)\n",
    "\n",
    "    dataframes = [\n",
    "        (X_train_SMOTE, Y_train_SMOTE, \"SMOTE Wrong Oversampling\"),\n",
    "        (X_train_ROS, Y_train_ROS, \"ROS Wrong Oversampling\"),\n",
    "    ]\n",
    "\n",
    "    result = {}\n",
    "    result[\"Combination_Name\"] = combination_name\n",
    "\n",
    "    for X, Y, name in dataframes:\n",
    "        train_inds, test_inds = next(\n",
    "            StratifiedGroupKFold(n_splits=5).split(X, Y, groups=X.index)\n",
    "        )\n",
    "\n",
    "        X_train_int = X.iloc[train_inds]\n",
    "        X_test_int = X.iloc[test_inds]\n",
    "        Y_train_int = Y.iloc[train_inds]\n",
    "        Y_test_int = Y.iloc[test_inds]\n",
    "\n",
    "        X_test_ext = X_test\n",
    "        Y_test_ext = Y_test\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_int = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train_int), columns=X_train_int.columns\n",
    "        )\n",
    "        X_test_int = pd.DataFrame(\n",
    "            scaler.transform(X_test_int), columns=X_test_int.columns\n",
    "        )\n",
    "        X_test_ext = pd.DataFrame(\n",
    "            scaler.transform(X_test_ext), columns=X_test_ext.columns\n",
    "        )\n",
    "\n",
    "        features = LASSO_selection(X_train_int, Y_train_int, N_FEATURES)\n",
    "        X_train_int = X_train_int.loc[:, features]\n",
    "        X_test_int = X_test_int.loc[:, features]\n",
    "        X_test_ext = X_test_ext.loc[:, features]\n",
    "\n",
    "        plot_distribution(Y_train_int, f\"Internal Train Set {name}\", combination_name)\n",
    "\n",
    "        plot_distribution(Y_test_int, f\"Internal Test Set {name}\", combination_name)\n",
    "\n",
    "        plot_pca(\n",
    "            X_train_int,\n",
    "            Y_train_int,\n",
    "            selected_classes,\n",
    "            f\"Internal Train Set {name}\",\n",
    "            combination_name,\n",
    "            kernel_type=\"rbf\",\n",
    "            gamma_type=1,\n",
    "            alpha_type=0.1,\n",
    "        )\n",
    "\n",
    "        cv_inner = StratifiedGroupKFold(n_splits=5)\n",
    "        model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "        space = {\n",
    "            \"n_estimators\": [10, 100, 500, 1000],\n",
    "            \"max_features\": [\"sqrt\", \"auto\", 2, 4, 6],\n",
    "        }\n",
    "\n",
    "        search = GridSearchCV(model, space, scoring=\"f1_macro\", cv=cv_inner, refit=True)\n",
    "\n",
    "        res = search.fit(X_train_int, Y_train_int, groups=X_train_int.index)\n",
    "        best_model = res.best_estimator_\n",
    "\n",
    "        best_model.fit(X_train_int, Y_train_int)\n",
    "\n",
    "        # Calculate permutation score to test vs. random labels\n",
    "        score, perm_scores, pvalue = permutation_test_score(\n",
    "            best_model, X, Y, n_permutations=100, n_jobs=4, scoring=\"f1_macro\"\n",
    "        )\n",
    " \n",
    "        # Feature Importance\n",
    "        sorted_idx = best_model.feature_importances_.argsort()\n",
    "        plt.barh(\n",
    "            X_train_int.columns[sorted_idx],\n",
    "            best_model.feature_importances_[sorted_idx],\n",
    "            color=\"#083471\",\n",
    "        )\n",
    "        plt.xlabel(f\"{name} Feature Importance\")\n",
    "        plt.savefig(\n",
    "            os.path.join(RESULT_PATH, f\"Feature_Importance_{combination_name}_{name}.tiff\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        predictions_int = pd.Series(\n",
    "            pd.Categorical(best_model.predict(X_test_int), categories=selected_classes)\n",
    "        )\n",
    "        probabilities_int = best_model.predict_proba(X_test_int)\n",
    "        ytest_dummies_int = pd.get_dummies(Y_test_int, drop_first=False).to_numpy()\n",
    "        Y_test_int = pd.Series(\n",
    "            pd.Categorical(Y_test_int, categories=selected_classes),\n",
    "            index=Y_test_int.index,\n",
    "        )\n",
    "\n",
    "        conf_matrix_int = confusion_matrix(\n",
    "            Y_test_int, predictions_int, labels=selected_classes\n",
    "        )\n",
    "        array_int = np.array(conf_matrix_int)\n",
    "\n",
    "        plot_heatmap(\n",
    "            normalized_array=array_int,\n",
    "            classes=best_model.classes_,\n",
    "            conf_matrix=conf_matrix_int,\n",
    "            name=f\"{combination_name} Internal Test Set {name}\",\n",
    "            Y=Y_test_int,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        plot_multiclass_roc(\n",
    "            Y_test_int,\n",
    "            probabilities_int,\n",
    "            n_classes=len(selected_classes),\n",
    "            figsize=(6, 5),\n",
    "            name=f\"{combination_name} Internal Test Set {name}\",\n",
    "            classes=best_model.classes_,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "        result[f\"{name}_Length_Train\"] = np.shape(Y_train_int)[0]\n",
    "        result[f\"{name}_Length_Test_int\"] = np.shape(Y_test_int)[0]\n",
    "        result[f\"{name}_Length_Test_ext\"] = np.shape(Y_test_ext)[0]\n",
    "        result[f\"{name}_Cross_Validation_Score\"] = np.round(search.best_score_, 2)\n",
    "        result[f\"{name}_Permutation_pvalue\"] = np.round(pvalue, 4)\n",
    "        result[f\"{name}_Hyperparameters\"] = search.best_params_\n",
    "        result[f\"{name}_Features_Number\"] = len(features)\n",
    "        result[f\"{name}_Features\"] = features\n",
    "        result[f\"{name}_Model\"] = \"Random Forest Classifier\"\n",
    "        result[f\"{name}_Pred_Test_int\"] = predictions_int\n",
    "        result[f\"{name}_Proba_Test_int\"] = probabilities_int\n",
    "        result[f\"{name}_ACC_int\"] = np.round(\n",
    "            accuracy_score(Y_test_int, predictions_int), 2\n",
    "        )\n",
    "        result[f\"{name}_F1_int\"] = np.round(\n",
    "            f1_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_PREC_int\"] = np.round(\n",
    "            precision_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_REC_int\"] = np.round(\n",
    "            recall_score(Y_test_int, predictions_int, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_AUC_macro_ovo_int\"] = np.round(\n",
    "            roc_auc_score(\n",
    "                Y_test_int,\n",
    "                probabilities_int,\n",
    "                average=\"macro\",\n",
    "                multi_class=\"ovo\",\n",
    "                labels=sorted(selected_classes),\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        result[f\"{name}_log_loss_int\"] = log_loss(\n",
    "            Y_test_int, probabilities_int, labels=sorted(selected_classes)\n",
    "        )\n",
    "\n",
    "        predictions_ext = best_model.predict(X_test_ext)\n",
    "        probabilities_ext = best_model.predict_proba(X_test_ext)\n",
    "        ytest_dummies_ext = pd.get_dummies(Y_test_ext, drop_first=False).to_numpy()\n",
    "\n",
    "        conf_matrix_ext = confusion_matrix(\n",
    "            Y_test_ext, predictions_ext, labels=selected_classes\n",
    "        )\n",
    "        array_ext = np.array(conf_matrix_ext)\n",
    "\n",
    "        plot_heatmap(\n",
    "            normalized_array=array_ext,\n",
    "            classes=best_model.classes_,\n",
    "            conf_matrix=conf_matrix_ext,\n",
    "            name=f\"{combination_name} external test set {name}\",\n",
    "            Y=Y_test_ext,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        plot_multiclass_roc(\n",
    "            Y_test_ext,\n",
    "            probabilities_ext,\n",
    "            n_classes=5,\n",
    "            figsize=(6, 5),\n",
    "            name=f\"{combination_name} external test set {name}\",\n",
    "            classes=best_model.classes_,\n",
    "            combination=combination_name,\n",
    "        )\n",
    "\n",
    "        result[f\"{name}_Conf_Matrix_ext\"] = conf_matrix_ext\n",
    "        result[f\"{name}_Pred_Test_ext\"] = predictions_ext\n",
    "        result[f\"{name}_Proba_Test_ext\"] = probabilities_ext\n",
    "        result[f\"{name}_ACC_ext\"] = np.round(\n",
    "            accuracy_score(Y_test_ext, predictions_ext), 2\n",
    "        )\n",
    "        result[f\"{name}_F1_ext\"] = np.round(\n",
    "            f1_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_PREC_ext\"] = np.round(\n",
    "            precision_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_REC_ext\"] = np.round(\n",
    "            recall_score(Y_test_ext, predictions_ext, average=\"macro\"), 2\n",
    "        )\n",
    "        result[f\"{name}_AUC_macro_ovo_ext\"] = np.round(\n",
    "            roc_auc_score(\n",
    "                Y_test_ext,\n",
    "                probabilities_ext,\n",
    "                average=\"macro\",\n",
    "                multi_class=\"ovo\",\n",
    "                labels=sorted(selected_classes),\n",
    "            ),\n",
    "            2,\n",
    "        )\n",
    "        result[f\"{name}_log_loss_ext\"] = log_loss(\n",
    "            Y_test_ext, probabilities_ext, labels=sorted(selected_classes)\n",
    "        )\n",
    "        result[f\"{name}_Conf_Matrix_ext\"] = conf_matrix_ext\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.to_excel(\n",
    "    os.path.join(RESULT_PATH, \"results_Wrong_Oversampling_combinations.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Evaluation incl. Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = \"NUMBER_OF_FEATURES_TO_SELECT\"\n",
    "\n",
    "combination_name = \"Train = Local+UCSF, Test = STANFORD\"\n",
    "train_data = pd.concat([data_train, data_test_2])\n",
    "test_data = data_test_1\n",
    "\n",
    "X_train = train_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "Y_train = train_data[\"Primary\"]\n",
    "\n",
    "X_test = test_data.drop(\"Primary\", axis=1, inplace=False).astype(float)\n",
    "Y_test = test_data[\"Primary\"]\n",
    "\n",
    "train_inds, test_inds = next(\n",
    "    StratifiedGroupKFold(n_splits=5).split(X_train, Y_train, groups=X_train.index)\n",
    ")\n",
    "\n",
    "X_train_int = X_train.iloc[train_inds]\n",
    "X_test_int = X_train.iloc[test_inds]\n",
    "Y_train_int = Y_train.iloc[train_inds]\n",
    "Y_test_int = Y_train.iloc[test_inds]\n",
    "\n",
    "X_test_ext = X_test.copy()\n",
    "Y_test_ext = Y_test.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_int = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train_int), columns=X_train_int.columns\n",
    ")\n",
    "X_test_int = pd.DataFrame(scaler.transform(X_test_int), columns=X_test_int.columns)\n",
    "X_test_ext = pd.DataFrame(scaler.transform(X_test_ext), columns=X_test_ext.columns)\n",
    "\n",
    "features = LASSO_selection(X_train_int, Y_train_int, N_FEATURES)\n",
    "X_train_int = X_train_int.loc[:, features]\n",
    "X_test_int = X_test_int.loc[:, features]\n",
    "X_test_ext = X_test_ext.loc[:, features]\n",
    "\n",
    "# Choose Oversampling Strategy\n",
    "\"\"\"SMOTE_Sampler = SMOTE(random_state=RANDOM_SEED, k_neighbors=4)\n",
    "X_train_int, Y_train_int = SMOTE_Sampler.fit_resample(\n",
    "   X_train_int, Y_train_int\n",
    ")\"\"\"\n",
    "\n",
    "ROS = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "X_train_int, Y_train_int = ROS.fit_resample(X_train_int, Y_train_int)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    max_features=2,\n",
    "    n_estimators=500,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "model.fit(X_train_int, Y_train_int)\n",
    "\n",
    "test_data = [\n",
    "    (\"internal test data\", X_test_int, Y_test_int),\n",
    "    (\"external test data\", X_test_ext, Y_test_ext),\n",
    "]\n",
    "\n",
    "for test_name, X_test, y_test in test_data:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "\n",
    "    n_iterations = 1000\n",
    "    labels = np.unique(y_test)\n",
    "    n_classes = len(labels)\n",
    "\n",
    "    original_metrics = {\n",
    "        \"accuracy\": {},\n",
    "        \"precision\": {},\n",
    "        \"recall\": {},\n",
    "        \"f1\": {},\n",
    "        \"roc_auc\": {},\n",
    "        \"accuracy_macro\": accuracy_score(y_test, y_pred),\n",
    "        \"precision_macro\": precision_score(\n",
    "            y_test, y_pred, average=\"macro\", zero_division=0\n",
    "        ),\n",
    "        \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"roc_auc_macro\": roc_auc_score(\n",
    "            y_test, y_prob, average=\"macro\", multi_class=\"ovo\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    label_to_index = {label: index for index, label in enumerate(labels)}\n",
    "    for label in labels:\n",
    "        is_label = y_test == label\n",
    "        if np.any(is_label):\n",
    "            index = label_to_index[label]\n",
    "            original_metrics[\"accuracy\"][label] = accuracy_score(\n",
    "                y_test[is_label], y_pred[is_label]\n",
    "            )\n",
    "            original_metrics[\"precision\"][label] = precision_score(\n",
    "                y_test, y_pred, labels=[label], average=\"macro\", zero_division=0\n",
    "            )\n",
    "            original_metrics[\"recall\"][label] = recall_score(\n",
    "                y_test, y_pred, labels=[label], average=\"macro\", zero_division=0\n",
    "            )\n",
    "            original_metrics[\"f1\"][label] = f1_score(\n",
    "                y_test, y_pred, labels=[label], average=\"macro\", zero_division=0\n",
    "            )\n",
    "            original_metrics[\"roc_auc\"][label] = roc_auc_score(\n",
    "                (y_test == label), y_prob[:, index]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Handle cases where a label might not be present in the test set\n",
    "            original_metrics[\"accuracy\"][label] = \"N/A\"\n",
    "            original_metrics[\"precision\"][label] = \"N/A\"\n",
    "            original_metrics[\"recall\"][label] = \"N/A\"\n",
    "            original_metrics[\"f1\"][label] = \"N/A\"\n",
    "            original_metrics[\"roc_auc\"][label] = \"N/A\"\n",
    "\n",
    "    bootstrap_metrics = {\n",
    "        \"accuracy\": {label: [] for label in labels},\n",
    "        \"precision\": {label: [] for label in labels},\n",
    "        \"recall\": {label: [] for label in labels},\n",
    "        \"f1\": {label: [] for label in labels},\n",
    "        \"roc_auc\": {label: [] for label in labels},\n",
    "        \"accuracy_macro\": [],\n",
    "        \"precision_macro\": [],\n",
    "        \"recall_macro\": [],\n",
    "        \"f1_macro\": [],\n",
    "        \"roc_auc_macro\": [],\n",
    "    }\n",
    "\n",
    "    for i in tqdm(range(n_iterations)):\n",
    "        bootstrap_indices = resample(\n",
    "            np.arange(len(y_test)), n_samples=len(y_test), random_state=i\n",
    "        )\n",
    "        y_true_sample = y_test[bootstrap_indices]\n",
    "        y_pred_sample = y_pred[bootstrap_indices]\n",
    "        y_prob_sample = y_prob[bootstrap_indices]\n",
    "        y_true_sample_binarized = y_true_sample\n",
    "\n",
    "        # Calculate metrics per label and macro\n",
    "        for label in labels:\n",
    "            is_label = y_true_sample == label\n",
    "            if np.any(is_label):\n",
    "                bootstrap_metrics[\"accuracy\"][label].append(\n",
    "                    accuracy_score(y_true_sample[is_label], y_pred_sample[is_label])\n",
    "                )\n",
    "                bootstrap_metrics[\"precision\"][label].append(\n",
    "                    precision_score(\n",
    "                        y_true_sample,\n",
    "                        y_pred_sample,\n",
    "                        labels=[label],\n",
    "                        average=\"macro\",\n",
    "                        zero_division=0,\n",
    "                    )\n",
    "                )\n",
    "                bootstrap_metrics[\"recall\"][label].append(\n",
    "                    recall_score(\n",
    "                        y_true_sample,\n",
    "                        y_pred_sample,\n",
    "                        labels=[label],\n",
    "                        average=\"macro\",\n",
    "                        zero_division=0,\n",
    "                    )\n",
    "                )\n",
    "                bootstrap_metrics[\"f1\"][label].append(\n",
    "                    f1_score(\n",
    "                        y_true_sample,\n",
    "                        y_pred_sample,\n",
    "                        labels=[label],\n",
    "                        average=\"macro\",\n",
    "                        zero_division=0,\n",
    "                    )\n",
    "                )\n",
    "                bootstrap_metrics[\"roc_auc\"][label].append(\n",
    "                    roc_auc_score(\n",
    "                        (y_true_sample == label),\n",
    "                        y_prob_sample[:, labels == label],\n",
    "                        average=\"macro\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        bootstrap_metrics[\"accuracy_macro\"].append(\n",
    "            accuracy_score(y_true_sample, y_pred_sample)\n",
    "        )\n",
    "        bootstrap_metrics[\"precision_macro\"].append(\n",
    "            precision_score(\n",
    "                y_true_sample, y_pred_sample, average=\"macro\", zero_division=0\n",
    "            )\n",
    "        )\n",
    "        bootstrap_metrics[\"recall_macro\"].append(\n",
    "            recall_score(y_true_sample, y_pred_sample, average=\"macro\", zero_division=0)\n",
    "        )\n",
    "        bootstrap_metrics[\"f1_macro\"].append(\n",
    "            f1_score(y_true_sample, y_pred_sample, average=\"macro\", zero_division=0)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            bootstrap_metrics[\"roc_auc_macro\"].append(\n",
    "                roc_auc_score(\n",
    "                    y_true_sample_binarized,\n",
    "                    y_prob_sample,\n",
    "                    average=\"macro\",\n",
    "                    multi_class=\"ovo\",\n",
    "                )\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            bootstrap_metrics[\"roc_auc\"][label].append(np.nan)\n",
    "\n",
    "    alpha = 0.95\n",
    "    confidence_intervals = {}\n",
    "    for metric, values in bootstrap_metrics.items():\n",
    "        if metric in bootstrap_metrics.keys() - {\n",
    "            \"accuracy_macro\",\n",
    "            \"precision_macro\",\n",
    "            \"recall_macro\",\n",
    "            \"f1_macro\",\n",
    "            \"roc_auc_macro\",\n",
    "        }:  # Label-based metrics\n",
    "            confidence_intervals[metric] = {}\n",
    "            for label, scores in values.items():\n",
    "                lower = np.nanpercentile(scores, (1 - alpha) / 2 * 100)\n",
    "                upper = np.nanpercentile(scores, (alpha + (1 - alpha) / 2) * 100)\n",
    "                confidence_intervals[metric][label] = (lower, upper)\n",
    "        else:\n",
    "            lower = np.nanpercentile(values, (1 - alpha) / 2 * 100)\n",
    "            upper = np.nanpercentile(values, (alpha + (1 - alpha) / 2) * 100)\n",
    "            confidence_intervals[metric] = (lower, upper)\n",
    "\n",
    "    result_df = pd.DataFrame(columns=list(original_metrics.keys()))\n",
    "\n",
    "    for metric, values in original_metrics.items():\n",
    "        if metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "            for label, original_value in values.items():\n",
    "                ci = confidence_intervals[metric].get(label, (\"N/A\", \"N/A\"))\n",
    "                result_df.loc[\n",
    "                    label, metric\n",
    "                ] = f\"{original_value:.2f} [{ci[0]:.2f}, {ci[1]:.2f}]\"\n",
    "        else:\n",
    "            ci = confidence_intervals[metric]\n",
    "            result_df.loc[\n",
    "                \"All_Labels\", metric\n",
    "            ] = f\"{values:.2f} [{ci[0]:.2f}, {ci[1]:.2f}]\"\n",
    "\n",
    "    result_df.to_excel(RESULT_PATH, \"Final_Evaluation.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
